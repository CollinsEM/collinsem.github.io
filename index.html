<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
  </head>
  <body>
    <h1>collinsem.github.io</h1>
    <h2> HTM demos </h2>
    <ul>
      <li>
        <h3><a id="gataca" class="anchor" href="#gataca" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="gataca">HTM sequence learning over a 1D domain.</a></h3>
        
        <p>
          This demo shows the HTM temporal memory algorithm operating
          on a simple 1D domain containing only four possible input
          states.  The input states are for letters: A, C, G, T. Three
          sensor patches move over the domain. Each sensor patch
          consists of five sensors that directly encode the input
          state in one of four active neurons (bits) associated with
          each sensor. The last three neurons associated with each
          patch is an encoding of the next movement of the patch in
          one of three bits: left, stay, right.  These inputs are then
          incorporated into three temporal memory modules with eight
          neurons per column.
          
      </li>
      
      <li>
        <h3><a id="stereo-test" class="anchor" href="#stereo-test" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="stereo-test">Early stage visualization of an agent with binocular/stereo vision.</a></h3>
        
        <p>
          Prototype visualization of a simple agent in a simple
          environment. The agent possesses two cameras for visual
          input. The RGB channels are then digitized very coarsely and
          projected onto a pair of virtual retinas. This is of course
          not how the actual processing occurs in the retina. That is
          currently on the TODO list. The purpose of this
          visualization was to prototype a potential interactive
          application that would be able to show the initial stages of
          encoding and processing stereo vision.
          
      </li>
      
      <li>
        <h3><a id="maze-runner" class="anchor" href="#maze-runner" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="MazeRunner">2D Maze environment with rudimentary physics. Potential environment for agent.</h3></a>
        
        <p>
          Another candidate for a simple embodied agent: a spherical
          rat in a maze. This demo only got as far as implementing
          basic collision physics before getting bogged down in non-AI
          details. Going foward, I will probably utilize an existing
          physics engine and focus on how the agent generates movement
          and receives sensor feedback from its environment.
          
      </li>
      <li>
        <h3><a id="nnvis" class="anchor" href="#nnvis" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="nnvis">3D visualization of a simulated cortical column.</a></h3>
        
        <p>
          This demo is mostly pretty flashing lights that demonstrates
          one potential way to visualize the inner workings of a
          single cortical column. There was a half-hearted attempt to
          implement a temporal memory algorithm, and you can kind of
          see it working in the shifting of the neurons from red
          (active-bursting) to blue (predicted) and green
          (active-predicted). However, the proximal inputs at the
          lowest level are essentially random, so no meaningful
          learning is taking place.
          
      </li>
      
      <li>
        <h3><a id="mnist-sparse-rep" class="anchor" href="#mnist-sparse-rep" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="mnist-sparse-rep">Sparse encoding of MNIST digits using a simple dictionary lookup algorithm.</a></h3>
        
        <p>
          Atoms in the dictionary are initialized by sub-sampling from
          a set of random images in the training set.  Thereafter
          these atoms are used as an overcomplete basis set to encode
          portions of subsequent images. The encoding selects the best
          atom by direct projection (dot product of image and basis
          atom) to obtain a correlation coefficient. The product of
          this coefficient and the basis atom is subtracted from the
          image leaving a residual. This residual is then subjected to
          the same procedure to select the next atom that best
          captures the image features that were not present in the
          first atom. This continues until the atom limit is reached
          or the magnitude of the residual falls below a minimum
          threshold. The reconstructed image is then displayed along
          with the residual.
        <p>
          NOTE: This demo is not currently learning or adapting the
          atoms after the initial sampling stage. This simple choice
          for the basis set yields some fairly impressive results
          which can best be appreciated by comparing them to the
          reconstructions that results if you enable the "random
          atoms" checkbox in the menu.
          
      </li>
      
      <li>
        <h3><a id="vision-proto1" class="anchor" href="#vision-proto1" aria-hidden="true"><span class="octicon octicon-link"></span></a>
          <a href="https://vision-proto1.herokuapp.com/">Retina to V1 mapping</a></h3>

        <p>
          Prototype of a visualization of a low-level visual encoding
          strategy. At the top of the window a sequence of MNIST
          digits are displayed with an overlay of a stencil displaying
          the proximal receptive fields for a set of cortical
          columns. The size of the stencil can be controled through
          colRadius. 
        <p>
          The primary visualization is a 3D depiction of the encoding
          of features associated with each column's receptive
          fields. Each cortical column is composed of 19 mini-columns
          rendered as three concentric rings (1+6+12). The intensity
          of each mini-column corresponds to the strength with which
          the input field matches one of the nineteen Gabor Filters
          (show in the lower left corner).**
        <p>
          ** If the Gabor field is unchecked, then a simpler set of 6
          filters is used: centerOn, centerOff, xSobel, ySobel,
          xScharr, and yScharr.
        
      </li>

      <li>
        <h3><a id="stereo-vision-test1" class="anchor" href="#stereo-vision-test1" aria-hidden="true"><span class="octicon octicon-link"></span></a>
          <a href="https://stereo-vision-test1.herokuapp.com/">Stereo vision prototype with retinal encoding visualization</a></h3>
        
        <p>
          A more detailed stereo vision prototype. This one is
          designed to understand more about how to create a
          sensor-motor feedback loop for aligning two independent
          retinal sensor patches on the same location in the input
          field.

        <p>
          In this demo, two retinal patches are overlaid on an input
          field consisting of a sequence of colored MNIST digits. Each
          patch consists of multiple retinal sensors covering a fixed
          spatial extent. The individual circles rendered on the
          sequence display the receptive field of each retinal sensor.

        <p>
          The main portion of the display window shows the cortical
          regions associated with the two retinal patches. Each region
          consists of a cortical column for each retinal
          sensor. Within each column are multiple minicolumns. Each
          minicolumn is proximally connected to the column's sensor
          via a log-Gabor convolutional filter. This filter is
          currently standing in place for what will eventually become
          an adaptive (Hebbian) filter. The minicolumn with the
          greatest filter response (dot product of receptive field
          with log-Gabor filter) fires first and then decays over
          time. While it is fading, the next best filter match has the
          opportunity to fire. This process continues until either no
          more filters exceed the activation threshold, or any of the
          previously activated filters have completed their refractory
          period and are ready to fire again.

        <p>
          NOTE: Some of the controls are currently inactive as the
          backend functionality has not yet been completed.
      </li>
      
      <li>
        <h3><a id="htm-conway" class="anchor" href="#htm-conway" aria-hidden="true"><span class="octicon octicon-link"></span></a>
          <a href="htm-conway">Can HTM learn Conway's Game of Life?</a></h3>

        <p>
          Inspired by a question asked in the HTM Forum, this example
          seeks to answer the question: "Can HTM learn Conway's Game
          of Life?"

        <p>
          This is a work in progress. Check back soon for future
          updates.
          
    </ul>
    <h2> Notes </h2>
    <ul>
      <li>For demos showing simulated neurons and/or synapses, colors
      are indicative of the current state of the neuron.
        <ul>
          <li>Blue: predictive (high-probability of becoming active
          soon)</li>
          <li>Green: normal activation (activated by proximal input
          after being in the predictive state)</li>
          <li>Red: bursting (activated by proximal input without first
          being predicted)</li>
        </ul>
      </li>
    </ul>
  </body>
</html>
