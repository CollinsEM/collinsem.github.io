<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
  </head>
  <body>
    <h1>collinsem.github.io</h1>
    <h2> HTM demos </h2>
    <ul>
      <li>
        <a id="gataca" class="anchor" href="#gataca" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="gataca"><h3>HTM sequence learning over a 1D domain.</h3></a>
        <br>
        This demo shows the HTM temporal memory algorithm operating on a simple 1D domain containing only four possible input states.
        The input states are for letters: A, C, G, T. Three sensor patches move over the domain. Each sensor patch consists of five
        sensors that directly encode the input state in one of four active neurons (bits) associated with each sensor. The last three
        neurons associated with each patch is an encoding of the next movement of the patch in one of three bits: left, stay, right.
        These inputs are then incorporated into three temporal memory modules with eight neurons per column.
      </li>
      <li>
        <a id="stereo-test" class="anchor" href="#stereo-test" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="stereo-test"><h3>Early stage visualization of an agent with binocular/stereo vision.</h3></a>
        <br>
        Prototype visualization of a simple agent in a simple environment. The agent possesses two cameras for visual input. The RGB
        channels are then digitized very coarsely and projected onto a pair of virtual retinas. This is of course not how the actual
        processing occurs in the retina. That is currently on the TODO list. The purpose of this visualization was to prototype a
        potential interactive application that would be able to show the initial stages of encoding and processing stereo vision.
      </li>
      <li>
        <a id="maze-runner" class="anchor" href="#maze-runner" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="MazeRunner"><h3>2D Maze environment with rudimentary physics. Potential environment for agent.</h3></a>
        <br>
        Another candidate for a simple embodied agent: a spherical rat in a maze. This demo only got as far as implementing basic collision
        physics before getting bogged down in non-AI details. Going foward, I will probably utilize an existing physics engine and focus
        on how the agent generates movement and receives sensor feedback from its environment.
      </li>
      <li>
        <a id="nnvis" class="anchor" href="#nnvis" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="nnvis"><h3>3D visualization of a simulated cortical column.</h3></a>
        <br>
        This demo is mostly pretty flashing lights that demonstrates one potential way to visualize the inner workings of a single cortical
        column. There was a half-hearted attempt to implement a temporal memory algorithm, and you can kind of see it working in the shifting
        of the neurons from red (active-bursting) to blue (predicted) and green (active-predicted). However, the proximal inputs at the
        lowest level are essentially random, so no meaningful learning is taking place.
      </li>
      <li>
        <a id="mnist-sparse-rep" class="anchor" href="#mnist-sparse-rep" aria-hidden="true"><span class="octicon octicon-link"></span></a>
        <a href="mnist-sparse-rep"><h3>Sparse encoding of MNIST digits using a simple dictionary lookup algorithm.</h3></a>
        <br>
        Atoms in the dictionary are initialized by sub-sampling from a set of random images in the training set. 
        Thereafter these atoms are used as an overcomplete basis set to encode portions of subsequent images. The encoding selects
        the best atom by direct projection (dot product of image and basis atom) to obtain a correlation coefficient. The product of this
        coefficient and the basis atom is subtracted from the image leaving a residual. This residual is then subjected to the same procedure
        to select the next atom that best captures the image features that were not present in the first atom. This continues until the atom
        limit is reached or the magnitude of the residual falls below a minimum threshold. The reconstructed image is then displayed along with
        the residual.
        <br>
        NOTE: This demo is not currently learning or adapting the atoms after the initial sampling stage. This simple choice for the basis set
        yields some fairly impressive results which can best be appreciated by comparing them to the reconstructions that results if you enable
        the "random atoms" checkbox in the menu.
      </li>
    </ul>
    <h2> Notes </h2>
    <ul>
      <li>For demos showing simulated neurons and/or synapses, colors are indicative of the current state of the neuron.
        <ul>
          <li>Blue: predictive (high-probability of becoming active soon)</li>
          <li>Green: normal activation (activated by proximal input after being in the predictive state)</li>
          <li>Red: bursting (activated by proximal input without first being predicted)</li>
        </ul>
      </li>
    </ul>
  </body>
</html>
